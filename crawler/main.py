#!/usr/bin/env python3
"""
IC123 çˆ¬è™«ç³»ç»Ÿä¸»ç¨‹åº
æ”¯æŒæ‰‹åŠ¨è¿è¡Œå’Œå®šæ—¶è°ƒåº¦ä¸¤ç§æ¨¡å¼
"""

import asyncio
import argparse
import sys
from datetime import datetime
from loguru import logger

from scheduler import CrawlerScheduler
from scrapers.news_scraper import run_news_scraper
from scrapers.website_checker import run_website_checker
from scrapers.iccircle_scraper import run_iccircle_scraper
from utils.database import db

def setup_logger(log_level: str = "INFO"):
    """è®¾ç½®æ—¥å¿—"""
    logger.remove()
    logger.add(
        sys.stdout,
        level=log_level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    logger.add(
        "logs/crawler_{time:YYYY-MM-DD}.log",
        rotation="1 day",
        retention="30 days",
        level=log_level,
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}"
    )

async def run_news_task():
    """è¿è¡Œæ–°é—»çˆ¬å–ä»»åŠ¡"""
    logger.info("ğŸš€ Starting news scraping task")
    
    try:
        results = await run_news_scraper()
        
        logger.info("ğŸ“Š News scraping results:")
        total_items = 0
        for source, count in results.items():
            logger.info(f"  - {source}: {count} items")
            total_items += count
        
        logger.success(f"âœ… News scraping completed. Total: {total_items} items")
        return results
        
    except Exception as e:
        logger.error(f"âŒ News scraping failed: {e}")
        raise

async def run_website_task():
    """è¿è¡Œç½‘ç«™æ£€æŸ¥ä»»åŠ¡"""
    logger.info("ğŸ” Starting website checking task")
    
    try:
        results = await run_website_checker()
        
        logger.info("ğŸ“Š Website checking results:")
        logger.info(f"  - Total checked: {results['total_checked']}")
        logger.info(f"  - Available: {results['available']}")
        logger.info(f"  - Unavailable: {results['unavailable']}")
        
        if results['errors']:
            logger.warning(f"  - Errors: {len(results['errors'])}")
            for error in results['errors']:
                logger.warning(f"    â€¢ {error['website']}: {error['error']}")
        
        logger.success("âœ… Website checking completed")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Website checking failed: {e}")
        raise

async def run_iccircle_task():
    """è¿è¡ŒICæŠ€æœ¯åœˆçˆ¬è™«ä»»åŠ¡"""
    logger.info("ğŸ¯ Starting IC Circle scraping task")
    
    try:
        results = await run_iccircle_scraper()
        
        logger.info("ğŸ“Š IC Circle scraping results:")
        logger.info(f"  - WeChat accounts: {results['wechat_accounts']} items")
        
        logger.success(f"âœ… IC Circle scraping completed. Total: {results['wechat_accounts']} accounts")
        return results
        
    except Exception as e:
        logger.error(f"âŒ IC Circle scraping failed: {e}")
        raise

async def run_ai_summary_task():
    """è¿è¡ŒAIæ¦‚è¦ç”Ÿæˆä»»åŠ¡"""
    logger.info("ğŸ¤– Starting AI summary generation task")
    
    try:
        processed_count = await db.batch_process_ai_summaries(batch_size=10)
        
        logger.info("ğŸ“Š AI summary generation results:")
        logger.info(f"  - Processed: {processed_count} items")
        
        logger.success(f"âœ… AI summary generation completed. Total: {processed_count} items")
        return {"processed_count": processed_count}
        
    except Exception as e:
        logger.error(f"âŒ AI summary generation failed: {e}")
        raise

async def run_cleanup_task():
    """è¿è¡Œæ•°æ®æ¸…ç†ä»»åŠ¡"""
    logger.info("ğŸ§¹ Starting database cleanup task")
    
    try:
        # æ˜¾ç¤ºæ¸…ç†å‰çš„ç»Ÿè®¡
        stats_before = await db.get_database_stats()
        logger.info("ğŸ“Š Database stats before cleanup:")
        for table, count in stats_before.items():
            logger.info(f"  - {table}: {count} records")
        
        # æ¸…ç†é‡å¤çš„æ–°é—»
        news_cleaned = await db.clean_duplicate_news()
        
        # æ¸…ç†é‡å¤çš„ç½‘ç«™
        websites_cleaned = await db.clean_duplicate_websites()
        
        # æ¸…ç†é‡å¤çš„å¾®ä¿¡å…¬ä¼—å·
        wechat_cleaned = await db.clean_duplicate_wechat_accounts()
        
        # åˆ é™¤ä¸å¯ç”¨çš„ç½‘ç«™
        inactive_websites_deleted = await db.delete_inactive_websites()
        
        # æ˜¾ç¤ºæ¸…ç†åçš„ç»Ÿè®¡
        stats_after = await db.get_database_stats()
        logger.info("ğŸ“Š Database stats after cleanup:")
        for table, count in stats_after.items():
            logger.info(f"  - {table}: {count} records")
        
        logger.success(f"âœ… Cleanup completed. Removed {news_cleaned} duplicate news, {websites_cleaned} duplicate websites, {wechat_cleaned} duplicate WeChat accounts, and {inactive_websites_deleted} inactive websites")
        return {
            "news_cleaned": news_cleaned, 
            "websites_cleaned": websites_cleaned,
            "wechat_cleaned": wechat_cleaned,
            "inactive_websites_deleted": inactive_websites_deleted
        }
        
    except Exception as e:
        logger.error(f"âŒ Database cleanup failed: {e}")
        raise

async def run_remove_inactive_task():
    """è¿è¡Œåˆ é™¤ä¸å¯ç”¨ç½‘ç«™ä»»åŠ¡"""
    logger.info("ğŸ—‘ï¸ Starting inactive websites removal task")
    
    try:
        # æ˜¾ç¤ºåˆ é™¤å‰çš„ç»Ÿè®¡
        stats_before = await db.get_database_stats()
        logger.info("ğŸ“Š Database stats before removal:")
        for table, count in stats_before.items():
            logger.info(f"  - {table}: {count} records")
        
        # åˆ é™¤ä¸å¯ç”¨çš„ç½‘ç«™
        deleted_count = await db.delete_inactive_websites()
        
        # æ˜¾ç¤ºåˆ é™¤åçš„ç»Ÿè®¡
        stats_after = await db.get_database_stats()
        logger.info("ğŸ“Š Database stats after removal:")
        for table, count in stats_after.items():
            logger.info(f"  - {table}: {count} records")
        
        logger.success(f"âœ… Inactive websites removal completed. Deleted {deleted_count} websites")
        return {"inactive_websites_deleted": deleted_count}
        
    except Exception as e:
        logger.error(f"âŒ Inactive websites removal failed: {e}")
        raise

async def run_update_task():
    """è¿è¡Œå®Œæ•´æ•°æ®æ›´æ–°ä»»åŠ¡ï¼ˆæ¸…ç†+çˆ¬å–ï¼‰"""
    logger.info("ğŸ”„ Starting complete data update task")
    
    try:
        # 1. å…ˆæ¸…ç†é‡å¤æ•°æ®
        logger.info("Step 1: Cleaning duplicate data...")
        cleanup_results = await run_cleanup_task()
        
        await asyncio.sleep(2)  # çŸ­æš‚ç­‰å¾…
        
        # 2. çˆ¬å–æœ€æ–°æ–°é—»
        logger.info("Step 2: Fetching latest news...")
        news_results = await run_news_task()
        
        await asyncio.sleep(2)  # çŸ­æš‚ç­‰å¾…
        
        # 3. çˆ¬å–ICæŠ€æœ¯åœˆå…¬ä¼—å·
        logger.info("Step 3: Fetching IC Circle WeChat accounts...")
        iccircle_results = await run_iccircle_task()
        
        await asyncio.sleep(2)  # çŸ­æš‚ç­‰å¾…
        
        # 4. ç”ŸæˆAIæ¦‚è¦
        logger.info("Step 4: Generating AI summaries...")
        ai_summary_results = await run_ai_summary_task()
        
        await asyncio.sleep(2)  # çŸ­æš‚ç­‰å¾…
        
        # 5. æ£€æŸ¥ç½‘ç«™çŠ¶æ€
        logger.info("Step 5: Checking website status...")
        website_results = await run_website_task()
        
        logger.success("âœ… Complete data update finished")
        return {
            "cleanup": cleanup_results,
            "news": news_results,
            "iccircle": iccircle_results,
            "ai_summary": ai_summary_results,
            "websites": website_results
        }
        
    except Exception as e:
        logger.error(f"âŒ Data update failed: {e}")
        raise

def run_scheduler():
    """è¿è¡Œå®šæ—¶è°ƒåº¦å™¨"""
    logger.info("â° Starting crawler scheduler")
    
    try:
        scheduler = CrawlerScheduler()
        
        # æ˜¾ç¤ºè°ƒåº¦çŠ¶æ€
        status = scheduler.get_schedule_status()
        logger.info(f"ğŸ“… Scheduler configuration:")
        logger.info(f"  - News scraping hours: {status['news_hours']}")
        logger.info(f"  - Website check interval: every {status['website_check_days']} days")
        logger.info(f"  - Total scheduled jobs: {status['total_jobs']}")
        
        logger.info("ğŸ¯ Scheduler is running. Press Ctrl+C to stop.")
        scheduler.start()
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Scheduler stopped by user")
    except Exception as e:
        logger.error(f"âŒ Scheduler error: {e}")
        raise

async def show_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    logger.info("ğŸ“‹ IC123 Crawler System Status")
    
    try:
        # è·å–æ•°æ®åº“è¿æ¥çŠ¶æ€
        categories = await db.get_categories()
        logger.info(f"  - Database: âœ… Connected ({len(categories)} categories)")
        
        # è·å–æ•°æ®åº“ç»Ÿè®¡
        stats = await db.get_database_stats()
        logger.info("ğŸ“Š Database statistics:")
        for table, count in stats.items():
            logger.info(f"  - {table}: {count} records")
        
        # è·å–æœ€è¿‘çš„æ–°é—»æ•°é‡
        recent_titles = await db.get_recent_news_titles(1)
        logger.info(f"  - Recent news (24h): {len(recent_titles)} items")
        
        # è·å–ç½‘ç«™æ•°é‡
        websites = await db.get_websites_for_check()
        logger.info(f"  - Websites to monitor: {len(websites)}")
        
        logger.success("âœ… System status check completed")
        
    except Exception as e:
        logger.error(f"âŒ Status check failed: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="IC123 Crawler System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py news           # Run news scraping once
  python main.py websites       # Check all websites once  
  python main.py iccircle       # Scrape IC Circle WeChat accounts
  python main.py ai-summary     # Generate AI summaries for news
  python main.py cleanup        # Clean duplicate data
  python main.py remove-inactive # Remove inactive websites
  python main.py update         # Complete update (cleanup + scraping)
  python main.py schedule       # Start scheduled crawler
  python main.py status         # Show system status
  python main.py --log-level DEBUG news  # Run with debug logging
        """
    )
    
    parser.add_argument(
        'command',
        choices=['news', 'websites', 'iccircle', 'ai-summary', 'cleanup', 'remove-inactive', 'update', 'schedule', 'status'],
        help='Command to execute'
    )
    
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='Set logging level (default: INFO)'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logger(args.log_level)
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    logger.info("ğŸ¯ IC123 Crawler System")
    logger.info(f"ğŸ“… Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸ”§ Command: {args.command}")
    logger.info(f"ğŸ“ Log level: {args.log_level}")
    logger.info("-" * 50)
    
    try:
        if args.command == 'news':
            asyncio.run(run_news_task())
            
        elif args.command == 'websites':
            asyncio.run(run_website_task())
            
        elif args.command == 'iccircle':
            asyncio.run(run_iccircle_task())
            
        elif args.command == 'ai-summary':
            asyncio.run(run_ai_summary_task())
            
        elif args.command == 'cleanup':
            asyncio.run(run_cleanup_task())
            
        elif args.command == 'remove-inactive':
            asyncio.run(run_remove_inactive_task())
            
        elif args.command == 'update':
            asyncio.run(run_update_task())
            
        elif args.command == 'schedule':
            run_scheduler()
            
        elif args.command == 'status':
            asyncio.run(show_status())
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Operation interrupted by user")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"âŒ Operation failed: {e}")
        sys.exit(1)
    
    logger.info("ğŸ‰ Operation completed successfully")

if __name__ == "__main__":
    main()